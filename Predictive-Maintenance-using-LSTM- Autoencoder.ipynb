{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Predictive-Maintenance-using-LSTM- Autoencoder.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SPczV6fleYXN","colab_type":"code","outputId":"a4466e54-0486-46fb-a632-7e924b51851d","executionInfo":{"status":"ok","timestamp":1578997208005,"user_tz":-60,"elapsed":61218,"user":{"displayName":"Zvonimir Dujmić","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDAgZF-_hRuSik5ICxze3zKmVE0SvORZST1nHrChwc=s64","userId":"12344281135211920744"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9i6je6U2aNSn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"5cba46d9-0c59-494e-df19-2a10a3357a3d","executionInfo":{"status":"ok","timestamp":1578997211142,"user_tz":-60,"elapsed":3125,"user":{"displayName":"Zvonimir Dujmić","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDAgZF-_hRuSik5ICxze3zKmVE0SvORZST1nHrChwc=s64","userId":"12344281135211920744"}}},"source":["import keras\n","import keras.backend as K\n","from keras.layers.core import Activation\n","from keras.models import Sequential,load_model, Model\n","from keras.layers import Dense, Dropout, LSTM, Input, RepeatVector, TimeDistributed, Flatten\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn import preprocessing"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ekZJnWwoaSKL","colab_type":"code","colab":{}},"source":["# Setting seed for reproducibility\n","np.random.seed(1234)  \n","PYTHONHASHSEED = 0\n","\n","# define path to save model\n","model_path = '/content/drive/My Drive/Fax/Master Theses/Output/regression_model.h5'\n","output_path ='/content/drive/My Drive/Fax/Master Theses/Output'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjyXumaMadXk","colab_type":"code","colab":{}},"source":["##################################\n","# Data Ingestion\n","##################################\n","\n","# read training data - It is the aircraft engine run-to-failure data.\n","train_df = pd.read_csv('/content/drive/My Drive/Fax/Master Theses/Data/CMAPSSData/train_FD001.txt', sep=\" \", header=None)\n","train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n","train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","\n","train_df = train_df.sort_values(['id','cycle'])\n","\n","# read test data - It is the aircraft engine operating data without failure events recorded.\n","test_df = pd.read_csv('/content/drive/My Drive/Fax/Master Theses/Data/CMAPSSData/test_FD001.txt', sep=\" \", header=None)\n","test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n","test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","\n","# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n","truth_df = pd.read_csv('/content/drive/My Drive/Fax/Master Theses/Data/CMAPSSData/RUL_FD001.txt', sep=\" \", header=None)\n","truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oywMDm5jaz6l","colab_type":"code","colab":{}},"source":["#######\n","# TRAIN set\n","#######\n","# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n","rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n","rul.columns = ['id', 'max']\n","train_df = train_df.merge(rul, on=['id'], how='left')\n","train_df['RUL'] = train_df['max'] - train_df['cycle']\n","train_df.drop('max', axis=1, inplace=True)\n","\n","# generate label columns for training data\n","# we will only make use of \"label1\" for binary classification, \n","# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n","w1 = 30\n","w0 = 15\n","train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n","train_df['label2'] = train_df['label1']\n","train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n","\n","# MinMax normalization (from 0 to 1)\n","train_df['cycle_norm'] = train_df['cycle']\n","cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n","min_max_scaler = preprocessing.MinMaxScaler()\n","norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n","                             columns=cols_normalize, \n","                             index=train_df.index)\n","join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n","train_df = join_df.reindex(columns = train_df.columns)\n","\n","#train_df.to_csv('../../Dataset/PredictiveManteinanceEngineTraining.csv', encoding='utf-8',index = None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jcEV-ura6Fm","colab_type":"code","outputId":"b3bd6c6c-33db-4836-a81d-ecef82006103","executionInfo":{"status":"ok","timestamp":1578997214615,"user_tz":-60,"elapsed":1445,"user":{"displayName":"Zvonimir Dujmić","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDAgZF-_hRuSik5ICxze3zKmVE0SvORZST1nHrChwc=s64","userId":"12344281135211920744"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["\n","######\n","# TEST set\n","######\n","# MinMax normalization (from 0 to 1)\n","test_df['cycle_norm'] = test_df['cycle']\n","norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n","                            columns=cols_normalize, \n","                            index=test_df.index)\n","test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n","test_df = test_join_df.reindex(columns = test_df.columns)\n","test_df = test_df.reset_index(drop=True)\n","print(test_df.head())\n","\n","# We use the ground truth dataset to generate labels for the test data.\n","# generate column max for test data\n","rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n","rul.columns = ['id', 'max']\n","truth_df.columns = ['more']\n","truth_df['id'] = truth_df.index + 1\n","truth_df['max'] = rul['max'] + truth_df['more']\n","truth_df.drop('more', axis=1, inplace=True)\n","\n","# generate RUL for test data\n","test_df = test_df.merge(truth_df, on=['id'], how='left')\n","test_df['RUL'] = test_df['max'] - test_df['cycle']\n","test_df.drop('max', axis=1, inplace=True)\n","\n","# generate label columns w0 and w1 for test data\n","test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n","test_df['label2'] = test_df['label1']\n","test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n","\n","#test_df.to_csv('../../Dataset/PredictiveManteinanceEngineValidation.csv', encoding='utf-8',index = None)\n","\n","# pick a large window size of 50 cycles\n","sequence_length = 50\n","\n","# function to reshape features into (samples, time steps, features) \n","def gen_sequence(id_df, seq_length, seq_cols):\n","    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n","    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n","    we can use shorter ones \"\"\"\n","    # for one id I put all the rows in a single matrix\n","    data_matrix = id_df[seq_cols].values\n","    num_elements = data_matrix.shape[0]\n","    # Iterate over two lists in parallel.\n","    # For example id1 have 192 rows and sequence_length is equal to 50\n","    # so zip iterate over two following list of numbers (0,112),(50,192)\n","    # 0 50 -> from row 0 to row 50\n","    # 1 51 -> from row 1 to row 51\n","    # 2 52 -> from row 2 to row 52\n","    # ...\n","    # 111 191 -> from row 111 to 191\n","    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n","        yield data_matrix[start:stop, :]\n","        \n","# pick the feature columns \n","sensor_cols = ['s' + str(i) for i in range(1,22)]\n","sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n","sequence_cols.extend(sensor_cols)\n","\n","# TODO for debug \n","# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n","val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n","print(len(val))\n","\n","# generator for the sequences\n","# transform each id of the train dataset in a sequence\n","seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n","           for id in train_df['id'].unique())\n","\n","# generate sequences and convert to numpy array\n","seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n","print(seq_array.shape)\n","\n","# function to generate labels\n","def gen_labels(id_df, seq_length, label):\n","    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n","    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n","    we can use shorter ones \"\"\"\n","    # For one id I put all the labels in a single matrix.\n","    # For example:\n","    # [[1]\n","    # [4]\n","    # [1]\n","    # [5]\n","    # [9]\n","    # ...\n","    # [200]] \n","    data_matrix = id_df[label].values\n","    num_elements = data_matrix.shape[0]\n","    # I have to remove the first seq_length labels\n","    # because for one id the first sequence of seq_length size have as target\n","    # the last label (the previus ones are discarded).\n","    # All the next id's sequences will have associated step by step one label as target.\n","    return data_matrix[seq_length:num_elements, :]\n","\n","# generate labels\n","label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n","             for id in train_df['id'].unique()]\n","\n","label_array = np.concatenate(label_gen).astype(np.float32)\n","label_array.shape"],"execution_count":6,"outputs":[{"output_type":"stream","text":["   id  cycle  setting1  setting2  ...  s19       s20       s21  cycle_norm\n","0   1      1  0.632184  0.750000  ...  0.0  0.558140  0.661834     0.00000\n","1   1      2  0.344828  0.250000  ...  0.0  0.682171  0.686827     0.00277\n","2   1      3  0.517241  0.583333  ...  0.0  0.728682  0.721348     0.00554\n","3   1      4  0.741379  0.500000  ...  0.0  0.666667  0.662110     0.00831\n","4   1      5  0.580460  0.500000  ...  0.0  0.658915  0.716377     0.01108\n","\n","[5 rows x 27 columns]\n","142\n","(15631, 50, 25)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(15631, 1)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"UOzdJtsTbCnJ","colab_type":"code","outputId":"e5057a21-df51-4776-91ff-557a1aacacb3","executionInfo":{"status":"error","timestamp":1578997221883,"user_tz":-60,"elapsed":2951,"user":{"displayName":"Zvonimir Dujmić","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDAgZF-_hRuSik5ICxze3zKmVE0SvORZST1nHrChwc=s64","userId":"12344281135211920744"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["##################################\n","# Autoencoding\n","##################################\n","def r2_keras(y_true, y_pred):\n","    \"\"\"Coefficient of Determination \n","    \"\"\"\n","    SS_res =  K.sum(K.square( y_true - y_pred ))\n","    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n","    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n","\n","def autoencoder(data, latent_dim = 14):\n","\n","  n_samples = data.shape[0]\n","  timesteps = data.shape[1]\n","  n_features = data.shape[2]\n","  #n_samples x timesteps x n_features\n","\n","  inputs = Input(shape=(timesteps, n_features))\n","  encoded = LSTM(latent_dim)(inputs)\n","\n","  decoded = RepeatVector(timesteps)(encoded)\n","  decoded = LSTM(n_features, return_sequences=True)(decoded)\n","\n","  sequence_autoencoder = Model(inputs, decoded)\n","\n","  sequence_autoencoder.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n","\n","  print(sequence_autoencoder.summary())\n","\n","  # fit the network\n","  history = sequence_autoencoder.fit(seq_array, seq_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n","            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n","                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n","            )\n","  \n","\n","  # make model for encoder only \n","  model = Model(inputs=sequence_autoencoder.inputs, outputs=sequence_autoencoder.layers[1].output)\n","    \n","  return model\n","\n","\n","\n","\n","seq_in = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n","# reshape input into [samples, timesteps, features]\n","n_in = len(seq_in)\n","seq_in = seq_in.reshape((1, n_in, 1))\n","# prepare output sequence\n","seq_out = seq_in[:, 1:, :]\n","n_out = n_in - 1\n","# define encoder\n","visible = Input(shape=(n_in,1))\n","encoder = LSTM(100, activation='relu')(visible)\n","# define reconstruct decoder\n","decoder1 = RepeatVector(n_in)(encoder)\n","decoder1 = LSTM(100, activation='relu', return_sequences=True)(decoder1)\n","decoder1 = TimeDistributed(Dense(1))(decoder1)\n","# define predict decoder\n","decoder2 = RepeatVector(n_out)(encoder)\n","decoder2 = LSTM(100, activation='relu', return_sequences=True)(decoder2)\n","decoder2 = TimeDistributed(Dense(1))(decoder2)\n","# tie it together\n","model = Model(inputs=visible, outputs=[decoder1, decoder2])\n","model.compile(optimizer='adam', loss='mse')\n","plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')\n","# fit model\n","model.fit(seq_in, [seq_in,seq_out], epochs=300, verbose=0)\n","# demonstrate prediction\n","yhat = model.predict(seq_in, verbose=0)\n","print(yhat)"],"execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-29cdeb65ef1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mseq_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;31m# reshape input into [samples, timesteps, features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mn_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ciG_E2ogkWYo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}